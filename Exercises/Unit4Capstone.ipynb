{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn import ensemble\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('C://duh/Thinkful/Thinkful/Exercises/Medium_AggregatedData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['audioVersionDurationSec', 'codeBlock', 'codeBlockCount', 'collectionId', 'imageCount', 'isSubscriptionLocked', 'linksCount', 'postId', 'readingTime', 'responsesCreatedCount', 'socialRecommendsCount', 'tagsCount', 'publicationfacebookPageName', 'publicationfollowerCount', 'publicationpublicEmail', 'publicationdomain', 'publicationslug', 'publicationtwitterUsername', 'userId', 'userName', 'usersFollowedByCount', 'usersFollowedCount'], 1, inplace=True)\n",
    "df.drop(['createdDatetime', 'latestPublishedDate', 'vote', 'scrappedDate', 'latestPublishedDatetime', 'slug', 'name', 'postCount', 'bio', 'firstPublishedDatetime', 'uniqueSlug', 'updatedDate', 'updatedDatetime'], 1, inplace=True)\n",
    "df.drop(df.index[df.language != 'en'], inplace=True)\n",
    "df = df.reset_index(drop=True)\n",
    "authorindex = []\n",
    "for author in df.author.unique():\n",
    "    if df.text.where(df.author == author).nunique() >= 100:\n",
    "        authorindex.append(author)\n",
    "authorindex = authorindex[:10]\n",
    "df = df.query('author in @authorindex')\n",
    "df = df[:int(len(df)/100)]\n",
    "df = df.reset_index(drop=True)\n",
    "nlp = spacy.load('en')\n",
    "df['doc'] = None\n",
    "df['bow'] = None\n",
    "\n",
    "del authorindex\n",
    "\n",
    "sentences = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(df.text)):\n",
    "    if x % 250 == 0:\n",
    "        print(\"Processing row {}\".format(x))\n",
    "    #df.text[x] = re.sub(df.title[x], '', df.text[x])\n",
    "    df.text[x] = df.text[x].lower()\n",
    "    df.text[x] = re.sub('https://*.*?\\\\n', ' ', df.text[x])\n",
    "    df.text[x] = re.sub('http://*.*?\\\\n', ' ', df.text[x])\n",
    "    df.text[x] = re.sub('\\\\n', ' ', df.text[x])\n",
    "    df.text[x] = re.sub('bibliography.*', '', df.text[x])\n",
    "    df.text[x] = re.sub('[%s]' % re.escape(string.punctuation), '', df.text[x])\n",
    "    df.text[x] = re.sub('\\w*\\d\\w*', '', df.text[x])\n",
    "    df.text[x] = re.sub('’', '', df.text[x])\n",
    "    df.text[x] = re.sub('—', ' ', df.text[x])\n",
    "    df.text[x] = re.sub('”', '', df.text[x])\n",
    "    df.text[x] = re.sub('“', '', df.text[x])\n",
    "    df.text[x] = re.sub('‘', '', df.text[x])\n",
    "    df.text[x] = re.sub('€', '', df.text[x])\n",
    "    df.text[x] = re.sub('…', ' ', df.text[x])\n",
    "    df.text[x] = re.sub('\\ \\ ', ' ', df.text[x])\n",
    "    df.text[x] = df.text[x].strip()\n",
    "    if df.text[x] == '':\n",
    "        df.drop([x], inplace=True)\n",
    "        df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text[74]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(df.text)):\n",
    "    df.doc[x] = nlp(df.text[x])\n",
    "    sents = [[sent, df.author[x]] for sent in df.doc[x].sents]\n",
    "    sentences = sentences.append(sents, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(text):\n",
    "    \n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(df.text)):\n",
    "    df.bow[x] = bag_of_words(df.doc[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = set()\n",
    "for x in range(len(df.text)):\n",
    "    common_words.update(df.bow[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_counts = bow_features(sentences, common_words)\n",
    "print('A')\n",
    "a = bow_features(sentences[:int(len(sentences)/10)].reset_index(drop=True), common_words)\n",
    "print('B')\n",
    "b = bow_features(sentences[int(len(sentences)/10):int(len(sentences)/10)+4500].reset_index(drop=True), common_words)\n",
    "print('C')\n",
    "c = bow_features(sentences[int(len(sentences)/10)+4500:int(len(sentences)/10)+9000].reset_index(drop=True), common_words)\n",
    "print('D')\n",
    "d = bow_features(sentences[int(len(sentences)/10)+9000:int(len(sentences)/10)+13500].reset_index(drop=True), common_words)\n",
    "print('E')\n",
    "e = bow_features(sentences[int(len(sentences)/10)+13500:int(len(sentences)/10)+18000].reset_index(drop=True), common_words)\n",
    "print('F')\n",
    "f = bow_features(sentences[int(len(sentences)/10)+18000:int(len(sentences)/10)+22500].reset_index(drop=True), common_words)\n",
    "print('G')\n",
    "g = bow_features(sentences[int(len(sentences)/10)+22500:int(len(sentences)/10)+27000].reset_index(drop=True), common_words)\n",
    "print('H')\n",
    "h = bow_features(sentences[int(len(sentences)/10)+27000:int(len(sentences)/10)+31500].reset_index(drop=True), common_words)\n",
    "print('I')\n",
    "i = bow_features(sentences[int(len(sentences)/10)+31500:int(len(sentences)/10)+36000].reset_index(drop=True), common_words)\n",
    "print('J')\n",
    "j = bow_features(sentences[int(len(sentences)/10)+36000:].reset_index(drop=True), common_words)\n",
    "\n",
    "word_counts = word_counts.append([a, b, c, d, e, f, g, h, i, j], ignore_index=True)\n",
    "del a, b, c, d, e, f, g, h, i, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0,\n",
    "                                                   stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = normalize(X)\n",
    "\n",
    "X_pca = PCA(.95).fit_transform(X_norm)\n",
    "\n",
    "# Calculate predicted values.\n",
    "y_pred = KMeans(random_state=0).fit_predict(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_pred)\n",
    "plt.scatter(X_pca[:, 2], X_pca[:, 3], c=y_pred)\n",
    "plt.scatter(X_pca[:, 4], X_pca[:, 5], c=y_pred)\n",
    "plt.scatter(X_pca[:, 6], X_pca[:, 7], c=y_pred)\n",
    "plt.scatter(X_pca[:, 8], X_pca[:, 9], c=y_pred)\n",
    "plt.scatter(X_pca[:, 10], X_pca[:, 11], c=y_pred)\n",
    "plt.scatter(X_pca[:, 12], X_pca[:, 13], c=y_pred)\n",
    "plt.scatter(X_pca[:, 14], X_pca[:, 15], c=y_pred)\n",
    "plt.scatter(X_pca[:, 16], X_pca[:, 17], c=y_pred)\n",
    "plt.scatter(X_pca[:, 18], X_pca[:, 19], c=y_pred)\n",
    "plt.scatter(X_pca[:, 20], X_pca[:, 21], c=y_pred)\n",
    "plt.scatter(X_pca[:, 22], X_pca[:, 23], c=y_pred)\n",
    "plt.scatter(X_pca[:, 24], X_pca[:, 25], c=y_pred)\n",
    "plt.scatter(X_pca[:, 26], X_pca[:, 27], c=y_pred)\n",
    "plt.scatter(X_pca[:, 28], X_pca[:, 29], c=y_pred)\n",
    "plt.scatter(X_pca[:, 30], X_pca[:, 31], c=y_pred)\n",
    "plt.scatter(X_pca[:, 32], X_pca[:, 33], c=y_pred)\n",
    "plt.scatter(X_pca[:, 34], X_pca[:, 35], c=y_pred)\n",
    "plt.scatter(X_pca[:, 36], X_pca[:, 37], c=y_pred)\n",
    "plt.scatter(X_pca[:, 38], X_pca[:, 39], c=y_pred)\n",
    "plt.scatter(X_pca[:, 40], X_pca[:, 41], c=y_pred)\n",
    "plt.scatter(X_pca[:, 42], X_pca[:, 43], c=y_pred)\n",
    "plt.scatter(X_pca[:, 44], X_pca[:, 45], c=y_pred)\n",
    "plt.scatter(X_pca[:, 46], X_pca[:, 47], c=y_pred)\n",
    "plt.scatter(X_pca[:, 48], X_pca[:, 49], c=y_pred)\n",
    "plt.scatter(X_pca[:, 50], X_pca[:, 51], c=y_pred)\n",
    "plt.scatter(X_pca[:, 52], X_pca[:, 53], c=y_pred)\n",
    "plt.scatter(X_pca[:, 54], X_pca[:, 55], c=y_pred)\n",
    "plt.scatter(X_pca[:, 56], X_pca[:, 57], c=y_pred)\n",
    "plt.scatter(X_pca[:, 58], X_pca[:, 59], c=y_pred)\n",
    "plt.scatter(X_pca[:, 60], X_pca[:, 61], c=y_pred)\n",
    "plt.scatter(X_pca[:, 62], X_pca[:, 63], c=y_pred)\n",
    "plt.scatter(X_pca[:, 64], X_pca[:, 65], c=y_pred)\n",
    "plt.scatter(X_pca[:, 66], X_pca[:, 67], c=y_pred)\n",
    "plt.scatter(X_pca[:, 68], X_pca[:, 69], c=y_pred)\n",
    "plt.scatter(X_pca[:, 70], X_pca[:, 71], c=y_pred)\n",
    "plt.scatter(X_pca[:, 72], X_pca[:, 73], c=y_pred)\n",
    "plt.scatter(X_pca[:, 74], X_pca[:, 75], c=y_pred)\n",
    "plt.scatter(X_pca[:, 76], X_pca[:, 77], c=y_pred)\n",
    "plt.scatter(X_pca[:, 78], X_pca[:, 79], c=y_pred)\n",
    "plt.scatter(X_pca[:, 80], X_pca[:, 81], c=y_pred)\n",
    "plt.scatter(X_pca[:, 82], X_pca[:, 83], c=y_pred)\n",
    "plt.scatter(X_pca[:, 84], X_pca[:, 85], c=y_pred)\n",
    "plt.scatter(X_pca[:, 86], X_pca[:, 87], c=y_pred)\n",
    "plt.scatter(X_pca[:, 88], X_pca[:, 89], c=y_pred)\n",
    "plt.scatter(X_pca[:, 90], X_pca[:, 91], c=y_pred)\n",
    "plt.scatter(X_pca[:, 92], X_pca[:, 93], c=y_pred)\n",
    "plt.scatter(X_pca[:, 94], X_pca[:, 95], c=y_pred)\n",
    "plt.scatter(X_pca[:, 96], X_pca[:, 97], c=y_pred)\n",
    "plt.scatter(X_pca[:, 98], X_pca[:, 99], c=y_pred)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "        \n",
    "print('Training score: ', lr.score(X_train, y_train))\n",
    "print('Test score: ', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel='linear')\n",
    "train = svm.fit(X_train, y_train)\n",
    "\n",
    "print('Training score: ', svm.score(X_train, y_train))\n",
    "print('Test score: ', svm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5, \n",
    "                             min_df=2, \n",
    "                             stop_words='english', \n",
    "                             lowercase=True, \n",
    "                             use_idf=True,\n",
    "                             norm=u'l2',\n",
    "                             smooth_idf=True \n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[int(len(sentences)/10)+36000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
