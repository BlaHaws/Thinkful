{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn import ensemble\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C://duh/Thinkful/Thinkful/Exercises/Medium_AggregatedData.csv')\n",
    "df.drop(['audioVersionDurationSec', 'codeBlock', 'codeBlockCount', 'collectionId', 'imageCount', 'isSubscriptionLocked', 'linksCount', 'postId', 'readingTime', 'responsesCreatedCount', 'socialRecommendsCount', 'tagsCount', 'publicationfacebookPageName', 'publicationfollowerCount', 'publicationpublicEmail', 'publicationdomain', 'publicationslug', 'publicationtwitterUsername', 'userId', 'userName', 'usersFollowedByCount', 'usersFollowedCount'], 1, inplace=True)\n",
    "df.drop(['createdDatetime', 'latestPublishedDate', 'vote', 'scrappedDate', 'latestPublishedDatetime', 'slug', 'name', 'postCount', 'bio', 'firstPublishedDatetime', 'uniqueSlug', 'updatedDate', 'updatedDatetime'], 1, inplace=True)\n",
    "df.drop(df.index[df.language != 'en'], inplace=True)\n",
    "df = df.reset_index(drop=True)\n",
    "authorindex = []\n",
    "for author in df.author.unique():\n",
    "    if df.text.where(df.author == author).nunique() >= 100:\n",
    "        authorindex.append(author)\n",
    "df = df.query('author in @authorindex')\n",
    "df = df[:int(len(df)/10)]\n",
    "df = df.reset_index(drop=True)\n",
    "nlp = spacy.load('en')\n",
    "df['doc'] = None\n",
    "df['sents'] = None\n",
    "df['bow'] = None\n",
    "\n",
    "del authorindex\n",
    "\n",
    "sentences = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 1000\n"
     ]
    }
   ],
   "source": [
    "for x in range(len(df.text)):\n",
    "    if x % 1000 == 0:\n",
    "        print(\"Processing row {}\".format(x))\n",
    "    #df.text[x] = re.sub(df.title[x], '', df.text[x])\n",
    "    df.text[x] = df.text[x].lower()\n",
    "    df.text[x] = re.sub('https://*.*?\\\\n', ' ', df.text[x])\n",
    "    df.text[x] = re.sub('http://*.*?\\\\n', ' ', df.text[x])\n",
    "    df.text[x] = re.sub('\\\\n', ' ', df.text[x])\n",
    "    df.text[x] = re.sub('bibliography.*', '', df.text[x])\n",
    "    df.text[x] = re.sub('[%s]' % re.escape(string.punctuation), '', df.text[x])\n",
    "    df.text[x] = re.sub('\\w*\\d\\w*', '', df.text[x])\n",
    "    df.text[x] = re.sub('’', '', df.text[x])\n",
    "    df.text[x] = re.sub('–', ' ', df.text[x])\n",
    "    df.text[x] = df.text[x].strip()\n",
    "    if df.text[x] == '':\n",
    "        df.drop([x], inplace=True)\n",
    "        df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(df.text)):\n",
    "    df.doc[x] = nlp(df.text[x])\n",
    "    sents = [[sent, df.author[x]] for sent in df.doc[x].sents]\n",
    "    sentences = sentences.append(sents, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(text):\n",
    "    \n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(df.text)):\n",
    "    df.bow[x] = bag_of_words(df.doc[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = set()\n",
    "for x in range(len(df.text)):\n",
    "    common_words.update(df.bow[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "Processing row 0\n"
     ]
    }
   ],
   "source": [
    "#word_counts = bow_features(sentences, common_words)\n",
    "print('A')\n",
    "a = bow_features(sentences[:int(len(sentences)/10)].reset_index(drop=True), common_words)\n",
    "print('B')\n",
    "b = bow_features(sentences[int(len(sentences)/10):int(len(sentences)/10)+1000].reset_index(drop=True), common_words)\n",
    "print('C')\n",
    "c = bow_features(sentences[int(len(sentences)/10)+1000:int(len(sentences)/10)+2000].reset_index(drop=True), common_words)\n",
    "print('D')\n",
    "d = bow_features(sentences[int(len(sentences)/10)+2000:int(len(sentences)/10)+3000].reset_index(drop=True), common_words)\n",
    "print('E')\n",
    "e = bow_features(sentences[int(len(sentences)/10)+3000:int(len(sentences)/10)+4000].reset_index(drop=True), common_words)\n",
    "print('F')\n",
    "f = bow_features(sentences[int(len(sentences)/10)+4000:int(len(sentences)/10)+5000].reset_index(drop=True), common_words)\n",
    "print('G')\n",
    "g = bow_features(sentences[int(len(sentences)/10)+5000:int(len(sentences)/10)+6000].reset_index(drop=True), common_words)\n",
    "print('H')\n",
    "h = bow_features(sentences[int(len(sentences)/10)+6000:int(len(sentences)/10)+7000].reset_index(drop=True), common_words)\n",
    "print('I')\n",
    "i = bow_features(sentences[int(len(sentences)/10)+7000:int(len(sentences)/10)+8000].reset_index(drop=True), common_words)\n",
    "print('J')\n",
    "j = bow_features(sentences[int(len(sentences)/10)+8000:].reset_index(drop=True), common_words)\n",
    "\n",
    "print(\"here\")\n",
    "word_counts = word_counts.append([a, b, c, d, e, f, g, h, i, j], ignore_index=True)\n",
    "del a, b, c, d, e, f, g, h, i, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = normalize(X)\n",
    "\n",
    "X_pca = PCA(.95).fit_transform(X_norm)\n",
    "\n",
    "# Calculate predicted values.\n",
    "y_pred = KMeans(random_state=0).fit_predict(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_pred)\n",
    "plt.scatter(X_pca[:, 2], X_pca[:, 3], c=y_pred)\n",
    "plt.scatter(X_pca[:, 4], X_pca[:, 5], c=y_pred)\n",
    "plt.scatter(X_pca[:, 6], X_pca[:, 7], c=y_pred)\n",
    "plt.scatter(X_pca[:, 8], X_pca[:, 9], c=y_pred)\n",
    "#plt.scatter(X_pca[:, 10], X_pca[:, 11], c=y_pred)\n",
    "#plt.scatter(X_pca[:, 12], X_pca[:, 13], c=y_pred)\n",
    "#plt.scatter(X_pca[:, 14], X_pca[:, 15], c=y_pred)\n",
    "#plt.scatter(X_pca[:, 16], X_pca[:, 17], c=y_pred)\n",
    "#plt.scatter(X_pca[:, 18], X_pca[:, 19], c=y_pred)\n",
    "#plt.scatter(X_pca[:, 20], X_pca[:, 21], c=y_pred)\n",
    "#plt.scatter(X_pca[:, 22], X_pca[:, 23], c=y_pred)\n",
    "#plt.scatter(X_pca[:, 24], X_pca[:, 25], c=y_pred)\n",
    "#plt.scatter(X_pca[:, 26], X_pca[:, 27], c=y_pred)\n",
    "#plt.scatter(X_pca[:, 28], X_pca[:, 29], c=y_pred)\n",
    "#plt.scatter(X_pca[:, 30], X_pca[:, 31], c=y_pred)\n",
    "#plt.scatter(X_pca[:, 32], X_pca[:, 33], c=y_pred)\n",
    "#plt.scatter(X_pca[:, 34], X_pca[:, 35], c=y_pred)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "        \n",
    "print('Training score: ', lr.score(X_train, y_train))\n",
    "print('Test score: ', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel='linear')\n",
    "train = svm.fit(X_train, y_train)\n",
    "\n",
    "print('Training score: ', svm.score(X_train, y_train))\n",
    "print('Test score: ', svm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9565"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8956"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(sentences)/10)+8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
