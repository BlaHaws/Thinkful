{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Blaine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Blaine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "from sklearn import ensemble\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPModel():\n",
    "    \n",
    "    nlp = None\n",
    "    \n",
    "    df = None\n",
    "    \n",
    "    df_tfidf = None\n",
    "        \n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en')\n",
    "\n",
    "    def run_bow(self, corpus1, corpus1_auth, corpus2, corpus2_auth):\n",
    "        print(\"Using Bag of Words Analysis:\")\n",
    "        corpus1 = self.get_corpus(corpus1)\n",
    "        corpus2 = self.get_corpus(corpus2)\n",
    "\n",
    "        corpus1 = self.text_cleaner(corpus1)\n",
    "        corpus2 = self.text_cleaner(corpus2)\n",
    "        \n",
    "        #corpus1 = self.text_cleaner(corpus1[:int(len(corpus1)/10)])\n",
    "        #corpus2 = self.text_cleaner(corpus2[:int(len(corpus2)/10)])\n",
    "        \n",
    "        corpus1_doc = self.tokenize(corpus1)\n",
    "        corpus2_doc = self.tokenize(corpus2)\n",
    "        \n",
    "        corpus1_sents = self.sentences(corpus1_doc, corpus1_auth)\n",
    "        corpus2_sents = self.sentences(corpus2_doc, corpus2_auth)\n",
    "                \n",
    "        sentences = pd.DataFrame(corpus1_sents + corpus2_sents)\n",
    "                \n",
    "        corpus1_bow = self.bag_of_words(corpus1_doc)\n",
    "        corpus2_bow = self.bag_of_words(corpus2_doc)\n",
    "        \n",
    "        common_words = self.set_common_words(corpus1_bow, corpus2_bow)\n",
    "        \n",
    "        word_counts = self.bow_features(sentences, common_words)\n",
    "        \n",
    "        Y = word_counts['text_source']\n",
    "        X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=0)\n",
    "        \n",
    "        self.try_rfc(X_train, X_test, y_train, y_test)\n",
    "        self.try_lr(X_train, X_test, y_train, y_test)\n",
    "        self.try_clf(X_train, X_test, y_train, y_test)\n",
    "        self.try_svm(X_train, X_test, y_train, y_test)\n",
    "        \n",
    "    def get_corpus(self, corpus):\n",
    "        print('  Getting Novel')\n",
    "        corpus = gutenberg.raw(corpus)\n",
    "        return corpus\n",
    "    \n",
    "    def text_cleaner(self, text, verbose = True):\n",
    "        if verbose:\n",
    "            print('  Running Text Cleaner')\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'--',' ',text)\n",
    "        text = re.sub(r'\\*', '', text)\n",
    "        text = re.sub(r\"[\\[].*?[\\]]\", \"\", text)\n",
    "        text = re.sub(r'Chapter \\d+', '', text)\n",
    "        text = re.sub(r'CHAPTER .*', '', text)\n",
    "        text = re.sub(r'chapter .*', '', text)\n",
    "        text = re.sub(r'VOLUME \\w+', '', text)\n",
    "        text = re.sub(r'CHAPTER \\w+', '', text)\n",
    "        text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
    "        #text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, corpus, verbose = True):\n",
    "        if verbose:\n",
    "            print('  Tokenizing')\n",
    "        return self.nlp(corpus)\n",
    "    \n",
    "    def sentences(self, corpus, auth):\n",
    "        print('  Getting Sentences')\n",
    "        return [[sent, auth] for sent in corpus.sents]\n",
    "    \n",
    "    def bag_of_words(self, text):\n",
    "        print('  Running Bag of Words')\n",
    "        allwords = [token.lemma_\n",
    "                    for token in text\n",
    "                    if not token.is_punct\n",
    "                    and not token.is_stop]\n",
    "    \n",
    "        return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "    def set_common_words(self, corpus1, corpus2):\n",
    "        print('  Setting Common Words')\n",
    "        common_words = set(corpus1 + corpus2)\n",
    "        return common_words\n",
    "    \n",
    "    def bow_features(self, sentences, common_words):\n",
    "        print('  Running BoW Features')\n",
    "        df = pd.DataFrame(columns=common_words)\n",
    "        df['text_sentence'] = sentences[0]\n",
    "        df['text_source'] = sentences[1]\n",
    "        df.loc[:, common_words] = 0\n",
    "    \n",
    "        for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "            words = [token.lemma_\n",
    "                     for token in sentence\n",
    "                     if (\n",
    "                         not token.is_punct\n",
    "                         and not token.is_stop\n",
    "                         and token.lemma_ in common_words\n",
    "                     )]\n",
    "        \n",
    "            for word in words:\n",
    "                df.loc[i, word] += 1\n",
    "        \n",
    "            if i % 50 == 0:\n",
    "                print(\"    Processing row {}\".format(i))\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    def try_rfc(self, X_train, X_test, y_train, y_test):\n",
    "        print('  Training Random Forest Classifier')\n",
    "        rfc = ensemble.RandomForestClassifier()\n",
    "        \n",
    "        train = rfc.fit(X_train, y_train)\n",
    "        \n",
    "        print('    Training set score: {},\\n    Test set score: {},\\n    Cross Validation: {}'.format(\n",
    "            rfc.score(X_train, y_train), rfc.score(X_test, y_test), cross_val_score(rfc, X_train, y_train, cv=10)))\n",
    "\n",
    "    \n",
    "    def try_lr(self, X_train, X_test, y_train, y_test):\n",
    "        print('  Training Logistic Regression')\n",
    "        lr = LogisticRegression()\n",
    "        \n",
    "        train = lr.fit(X_train, y_train)\n",
    "        \n",
    "        print('    Training set score: {},\\n    Test set score: {},\\n    Cross Validation: {}'.format(\n",
    "            lr.score(X_train, y_train), lr.score(X_test, y_test), cross_val_score(lr, X_train, y_train, cv=10)))\n",
    "\n",
    "    \n",
    "    def try_clf(self, X_train, X_test, y_train, y_test):\n",
    "        print('  Training Boosted Classifier')\n",
    "        clf = ensemble.GradientBoostingClassifier()\n",
    "        \n",
    "        train = clf.fit(X_train, y_train)\n",
    "        \n",
    "        print('    Training set score: {},\\n    Test set score: {},\\n    Cross Validation: {}'.format(\n",
    "            clf.score(X_train, y_train), clf.score(X_test, y_test), cross_val_score(clf, X_train, y_train, cv=10)))\n",
    "    \n",
    "    def try_svm(self, X_train, X_test, y_train, y_test):\n",
    "        print('  Training Support Vector Machine')\n",
    "        svm = SVC(kernel='linear')\n",
    "        \n",
    "        train = svm.fit(X_train, y_train)\n",
    "        \n",
    "        print('    Training set score: {},\\n    Test set score: {},\\n    Cross Validation: {}'.format(\n",
    "            svm.score(X_train, y_train), svm.score(X_test, y_test), cross_val_score(svm, X_train, y_train, cv=10)))\n",
    "        \n",
    "    def run_tfidf(self, corpus1, corpus1_auth, corpus2, corpus2_auth):\n",
    "        print('Running tf-idf Analysis')\n",
    "        corpus1 = self.get_paragraphs(corpus1)\n",
    "        corpus2 = self.get_paragraphs(corpus2)\n",
    "        \n",
    "        corpus1 = self.clean_paragraph(corpus1)\n",
    "        corpus2 = self.clean_paragraph(corpus2)\n",
    "        \n",
    "        corpus1_df = self.build_dataframe(corpus1, corpus1_auth)\n",
    "        corpus2_df = self.build_dataframe(corpus2, corpus2_auth)\n",
    "        \n",
    "        self.df = corpus1_df.append(corpus2_df, ignore_index=True)\n",
    "                                                                 \n",
    "        print('  Creating Vectorizer (Unigram, and Bigram)')\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', lowercase=True, ngram_range=(1,2), use_idf=True,norm=u'l2', smooth_idf=True)\n",
    "        \n",
    "        print('  Vectorizing Paragraphs')\n",
    "        self.df_tfidf = vectorizer.fit_transform(self.df['Text'])\n",
    "        print('  Number of features: %d' % self.df_tfidf.get_shape()[1])\n",
    "        \n",
    "        self.df['TF-IDF'] = self.df_tfidf\n",
    "        \n",
    "        X = self.df_tfidf\n",
    "        Y = self.df['Author']\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=0)\n",
    "        \n",
    "        print('  Number of features used: 900')\n",
    "        svd= TruncatedSVD(900)\n",
    "        lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "\n",
    "        X_train_lsa = lsa.fit_transform(X_train)\n",
    "\n",
    "        variance_explained=svd.explained_variance_ratio_\n",
    "        total_variance = variance_explained.sum()\n",
    "        \n",
    "        print(\"  Percent variance captured by all components:\",total_variance*100)\n",
    "        \n",
    "        self.try_rfc(X_train, X_test, y_train, y_test)\n",
    "        self.try_lr(X_train, X_test, y_train, y_test)\n",
    "        self.try_clf(X_train, X_test, y_train, y_test)\n",
    "        self.try_svm(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    def get_paragraphs(self, corpus):\n",
    "        print('  Getting Novel (Paragraphs)')\n",
    "        corpus = gutenberg.paras(corpus)\n",
    "        corpus_paras=[]\n",
    "        for paragraph in corpus:\n",
    "            para=paragraph[0]\n",
    "            corpus_paras.append(' '.join(para))\n",
    "\n",
    "        return corpus_paras\n",
    "    \n",
    "    def clean_paragraph(self, corpus):\n",
    "        print('  Cleaning Paragraphs')\n",
    "        cleaned = [self.text_cleaner(para, False) for para in corpus]\n",
    "        reduced = [x for x in cleaned if x is not '']\n",
    "            \n",
    "        return reduced\n",
    "    \n",
    "    def build_dataframe(self, corpus, auth):\n",
    "        return pd.DataFrame([[para, auth] for para in corpus], columns=['Text', 'Author'])\n",
    "    \n",
    "    def get_tfidf(self, corpus):\n",
    "        print('  Getting TF-IDF for corpus')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Bag of Words Analysis:\n",
      "  Getting Novel\n",
      "  Getting Novel\n",
      "  Running Text Cleaner\n",
      "  Running Text Cleaner\n",
      "  Tokenizing\n",
      "  Tokenizing\n",
      "  Getting Sentences\n",
      "  Getting Sentences\n",
      "  Running Bag of Words\n",
      "  Running Bag of Words\n",
      "  Setting Common Words\n",
      "  Running BoW Features\n",
      "    Processing row 0\n",
      "    Processing row 50\n",
      "    Processing row 100\n",
      "    Processing row 150\n",
      "    Processing row 200\n",
      "    Processing row 250\n",
      "    Processing row 300\n",
      "    Processing row 350\n",
      "    Processing row 400\n",
      "    Processing row 450\n",
      "    Processing row 500\n",
      "    Processing row 550\n",
      "    Processing row 600\n",
      "    Processing row 650\n",
      "    Processing row 700\n",
      "    Processing row 750\n",
      "    Processing row 800\n",
      "    Processing row 850\n",
      "    Processing row 900\n",
      "    Processing row 950\n",
      "    Processing row 1000\n",
      "    Processing row 1050\n",
      "    Processing row 1100\n",
      "    Processing row 1150\n",
      "    Processing row 1200\n",
      "    Processing row 1250\n",
      "    Processing row 1300\n",
      "    Processing row 1350\n",
      "    Processing row 1400\n",
      "    Processing row 1450\n",
      "    Processing row 1500\n",
      "    Processing row 1550\n",
      "    Processing row 1600\n",
      "    Processing row 1650\n",
      "    Processing row 1700\n",
      "    Processing row 1750\n",
      "    Processing row 1800\n",
      "    Processing row 1850\n",
      "    Processing row 1900\n",
      "    Processing row 1950\n",
      "    Processing row 2000\n",
      "    Processing row 2050\n",
      "    Processing row 2100\n",
      "    Processing row 2150\n",
      "    Processing row 2200\n",
      "    Processing row 2250\n",
      "    Processing row 2300\n",
      "    Processing row 2350\n",
      "    Processing row 2400\n",
      "    Processing row 2450\n",
      "    Processing row 2500\n",
      "    Processing row 2550\n",
      "    Processing row 2600\n",
      "    Processing row 2650\n",
      "    Processing row 2700\n",
      "    Processing row 2750\n",
      "    Processing row 2800\n",
      "    Processing row 2850\n",
      "    Processing row 2900\n",
      "  Training Random Forest Classifier\n",
      "    Training set score: 0.985632183908046,\n",
      "    Test set score: 0.8742463393626184,\n",
      "    Cross Validation: [0.87428571 0.89142857 0.90857143 0.86206897 0.85632184 0.89655172\n",
      " 0.87931034 0.89595376 0.89595376 0.89017341]\n",
      "  Training Logistic Regression\n",
      "    Training set score: 0.9603448275862069,\n",
      "    Test set score: 0.9009474590869939,\n",
      "    Cross Validation: [0.89142857 0.92571429 0.90285714 0.8908046  0.89655172 0.8908046\n",
      " 0.9137931  0.92485549 0.90751445 0.90751445]\n",
      "  Training Boosted Classifier\n",
      "    Training set score: 0.8954022988505748,\n",
      "    Test set score: 0.8673557278208441,\n",
      "    Cross Validation: [0.85714286 0.90857143 0.89142857 0.86781609 0.85632184 0.86206897\n",
      " 0.85632184 0.89017341 0.83236994 0.87283237]\n",
      "  Training Support Vector Machine\n",
      "    Training set score: 0.9793103448275862,\n",
      "    Test set score: 0.8949181739879414,\n",
      "    Cross Validation: [0.89714286 0.92       0.90857143 0.88505747 0.88505747 0.8908046\n",
      " 0.91954023 0.9132948  0.89017341 0.90751445]\n",
      "Running tf-idf Analysis\n",
      "  Getting Novel (Paragraphs)\n",
      "  Getting Novel (Paragraphs)\n",
      "  Cleaning Paragraphs\n",
      "  Cleaning Paragraphs\n",
      "  Creating Vectorizer (Unigram, and Bigram)\n",
      "  Vectorizing Paragraphs\n",
      "  Number of features: 15571\n",
      "  Number of features used: 900\n",
      "  Percent variance captured by all components: 95.54439262836921\n",
      "  Training Random Forest Classifier\n",
      "    Training set score: 0.9796296296296296,\n",
      "    Test set score: 0.8543689320388349,\n",
      "    Cross Validation: [0.75229358 0.73148148 0.84259259 0.83333333 0.83333333 0.87037037\n",
      " 0.86111111 0.81481481 0.85185185 0.89719626]\n",
      "  Training Logistic Regression\n",
      "    Training set score: 0.9666666666666667,\n",
      "    Test set score: 0.8460471567267683,\n",
      "    Cross Validation: [0.78899083 0.82407407 0.88888889 0.85185185 0.7962963  0.78703704\n",
      " 0.87037037 0.83333333 0.85185185 0.85981308]\n",
      "  Training Boosted Classifier\n",
      "    Training set score: 0.8972222222222223,\n",
      "    Test set score: 0.8238557558945908,\n",
      "    Cross Validation: [0.75229358 0.81481481 0.85185185 0.83333333 0.78703704 0.83333333\n",
      " 0.82407407 0.82407407 0.82407407 0.82242991]\n",
      "  Training Support Vector Machine\n",
      "    Training set score: 0.9712962962962963,\n",
      "    Test set score: 0.8779472954230236,\n",
      "    Cross Validation: [0.85321101 0.87037037 0.89814815 0.88888889 0.89814815 0.84259259\n",
      " 0.92592593 0.87962963 0.87962963 0.89719626]\n"
     ]
    }
   ],
   "source": [
    "nlpmodel = NLPModel()\n",
    "nlpmodel.run_bow('carroll-alice.txt', 'Carroll', 'austen-persuasion.txt', 'Austen')\n",
    "nlpmodel.run_tfidf('carroll-alice.txt', 'Carroll', 'austen-persuasion.txt', 'Austen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "## Bag of Words Approach\n",
    "\n",
    "#### Random Forest Classifier\n",
    "Train Set: 98.6%\n",
    "\n",
    "Test Set: 87.4%\n",
    "\n",
    "Cross Validation Average: 88.5%\n",
    "\n",
    "#### Logistic Regression\n",
    "Train Set: 96.0%\n",
    "\n",
    "Test Set: 90.1%\n",
    "\n",
    "Cross Validation Average: 90.5%\n",
    "\n",
    "#### Boosted Classifier\n",
    "Train Set: 89.5%\n",
    "\n",
    "Test Set: 86.7%\n",
    "\n",
    "Cross Validation Average: 87.0%\n",
    "\n",
    "#### Support Vector Machine\n",
    "Train Set: 97.9%\n",
    "\n",
    "Test Set: 89.5%\n",
    "\n",
    "Cross Validation Average: 90.2%\n",
    "\n",
    "## TF-IDF Approach\n",
    "\n",
    "#### Random Forest Classifier\n",
    "Train Set: 98.0%\n",
    "\n",
    "Test Set: 85.4%\n",
    "\n",
    "Cross Validation Average: 83.5%\n",
    "\n",
    "#### Logistic Regression\n",
    "Train Set: 96.7%\n",
    "\n",
    "Test Set: 84.6%\n",
    "\n",
    "Cross Validation Average: 83.5%\n",
    "\n",
    "#### Boosted Classifier\n",
    "Train Set: 89.7%\n",
    "\n",
    "Test Set: 82.4%\n",
    "\n",
    "Cross Validation Average: 81.7%\n",
    "\n",
    "#### Support Vector Machine\n",
    "Train Set: 97.1%\n",
    "\n",
    "Test Set: 87.8%\n",
    "\n",
    "Cross Validation Average: 88.3%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Using a bag of words approach, we gather the 2000 most common per author (4000 total). Prediction on which author wrote the test documents is done against this large dataset. Logistic regression presents us with the best output overall for this approach. \n",
    "\n",
    "Using a TF-IDF approach, which incorporates vectorizing single words and bigrams, gives us a term frequency for each author. Prediction on which author wrote the test documents is done against this dataset. A support vector machine presents us with the best output for this approach.\n",
    "\n",
    "Logistic Regression with a bag of words approach seems to give us our best overall performance. This is supported with a difference of 5.9% between the training set score and test set score, as well as the highest cross validation score that almost matches the test set score.\n",
    "\n",
    "We can conclude that for this experiment, prediction using the most common words of an author is greater than using the words that are unique to the author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
