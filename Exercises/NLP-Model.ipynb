{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Blaine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Blaine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPModel():\n",
    "    \n",
    "    nlp = None\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en')\n",
    "\n",
    "    def run_bow(self, corpus1, corpus1_auth, corpus2, corpus2_auth):\n",
    "        print(\"Using Bag of Words Analysis:\")\n",
    "        corpus1 = self.get_corpus(corpus1)\n",
    "        corpus2 = self.get_corpus(corpus2)\n",
    "\n",
    "        corpus1 = self.text_cleaner(corpus1)\n",
    "        corpus2 = self.text_cleaner(corpus2)\n",
    "        \n",
    "        corpus1_doc = self.tokenize(corpus1)\n",
    "        corpus2_doc = self.tokenize(corpus2)\n",
    "        \n",
    "        corpus1_sents = self.sentences(corpus1_doc, corpus1_auth)\n",
    "        corpus2_sents = self.sentences(corpus2_doc, corpus2_auth)\n",
    "        \n",
    "        sentences = pd.DataFrame(corpus1_sents + corpus2_sents)\n",
    "        \n",
    "        corpus1_bow = self.bag_of_words(corpus1_doc)\n",
    "        corpus2_bow = self.bag_of_words(corpus2_doc)\n",
    "        \n",
    "        common_words = self.set_common_words(corpus1_bow, corpus2_bow)\n",
    "        \n",
    "        word_counts = self.bow_features(sentences, common_words)\n",
    "        \n",
    "        Y = word_counts['text_source']\n",
    "        X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=0)\n",
    "        \n",
    "        print(self.try_rfc(X_train, X_test, y_train, y_test))\n",
    "        print(self.try_lr(X_train, X_test, y_train, y_test))\n",
    "        print(self.try_clf(X_train, X_test, y_train, y_test))\n",
    "        \n",
    "    def get_corpus(self, corpus):\n",
    "        print('  Getting Novel')\n",
    "        corpus = gutenberg.raw(corpus)\n",
    "        return corpus\n",
    "    \n",
    "    def text_cleaner(self, text, verbose = True):\n",
    "        if verbose:\n",
    "            print('  Running Text Cleaner')\n",
    "        text = re.sub(r'--',' ',text)\n",
    "        text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "        text = re.sub(r'Chapter \\d+', '', text)\n",
    "        text = re.sub(r'CHAPTER .*', '', text)\n",
    "        text = re.sub(r'VOLUME \\w+', '', text)\n",
    "        text = re.sub(r'CHAPTER \\w+', '', text)\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, corpus, verbose = True):\n",
    "        if verbose:\n",
    "            print('  Tokenizing')\n",
    "        return self.nlp(corpus)\n",
    "    \n",
    "    def sentences(self, corpus, auth):\n",
    "        print('  Getting Sentences')\n",
    "        return [[sent, auth] for sent in corpus.sents]\n",
    "    \n",
    "    def bag_of_words(self, text):\n",
    "        print('  Running Bag of Words')\n",
    "        allwords = [token.lemma_\n",
    "                    for token in text\n",
    "                    if not token.is_punct\n",
    "                    and not token.is_stop]\n",
    "    \n",
    "        return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "    def bow_features(self, sentences, common_words):\n",
    "        print('  Running BoW Features')\n",
    "        df = pd.DataFrame(columns=common_words)\n",
    "        df['text_sentence'] = sentences[0]\n",
    "        df['text_source'] = sentences[1]\n",
    "        df.loc[:, common_words] = 0\n",
    "    \n",
    "        for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "            words = [token.lemma_\n",
    "                     for token in sentence\n",
    "                     if (\n",
    "                         not token.is_punct\n",
    "                         and not token.is_stop\n",
    "                         and token.lemma_ in common_words\n",
    "                     )]\n",
    "        \n",
    "            for word in words:\n",
    "                df.loc[i, word] += 1\n",
    "        \n",
    "            if i % 50 == 0:\n",
    "                print(\"    Processing row {}\".format(i))\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    def set_common_words(self, corpus1, corpus2):\n",
    "        print('  Setting Common Words')\n",
    "        common_words = set(corpus1 + corpus2)\n",
    "        return common_words\n",
    "    \n",
    "    def try_rfc(self, X_train, X_test, y_train, y_test):\n",
    "        print('  Training Random Forest Classifier')\n",
    "        rfc = ensemble.RandomForestClassifier()\n",
    "        \n",
    "        train = rfc.fit(X_train, y_train)\n",
    "        \n",
    "        results = '    Training set score:', rfc.score(X_train, y_train)\n",
    "        results += 'Test set score:', rfc.score(X_test, y_test)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def try_lr(self, X_train, X_test, y_train, y_test):\n",
    "        print('  Training Logistic Regression')\n",
    "        lr = LogisticRegression()\n",
    "        \n",
    "        train = lr.fit(X_train, y_train)\n",
    "        \n",
    "        results = '    Training set score:', lr.score(X_train, y_train)\n",
    "        results += 'Test set score:', lr.score(X_train, y_train)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def try_clf(self, X_train, X_test, y_train, y_test):\n",
    "        print('  Training Boosted Classifier')\n",
    "        clf = ensemble.GradientBoostingClassifier()\n",
    "        \n",
    "        train = clf.fit(X_train, y_train)\n",
    "        \n",
    "        results = '    Training set score:', clf.score(X_train, y_train)\n",
    "        results += 'Test set score:', clf.score(X_test, y_test)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def run_tfidf(self, corpus):\n",
    "        print('Running tf-idf Analysis')\n",
    "        corpus_paras = self.get_paragraphs(corpus)\n",
    "        cleaned_paras = self.clean_paragraphs(corpus_paras)\n",
    "        token_paras = self.tokenize_paragraphs(cleaned_paras)\n",
    "\n",
    "        X_train, X_test = train_test_split(cleaned_paras, test_size=0.4, random_state=0)\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(max_df=0.5,\n",
    "                                    min_df=2,\n",
    "                                    stop_words='english',\n",
    "                                    lowercase=True,\n",
    "                                    use_idf=True,\n",
    "                                    norm=u'l2',\n",
    "                                    smooth_idf=True)\n",
    "        print(\"  Vectorizer Built\")\n",
    "        \n",
    "        corpus_paras_tfidf=vectorizer.fit_transform(token_paras)\n",
    "        print(\"  Number of features: %d\" % corpus_paras_tfidf.get_shape()[1])\n",
    "        \n",
    "        X_train_tfidf, X_test_tfidf= train_test_split(corpus_paras_tfidf, test_size=0.4, random_state=0)\n",
    "        \n",
    "        X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "        n = X_train_tfidf_csr.shape[0]\n",
    "\n",
    "        tfidf_bypara = [{} for _ in range(0,n)]\n",
    "\n",
    "        terms = vectorizer.get_feature_names()\n",
    "\n",
    "        for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "            tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "        print('  Original sentence:', X_train[5])\n",
    "        print('  Tf_idf vector:', tfidf_bypara[5]) \n",
    "        \n",
    "        svd= TruncatedSVD(700)\n",
    "        lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "\n",
    "        X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "        \n",
    "        variance_explained=svd.explained_variance_ratio_\n",
    "\n",
    "        total_variance = variance_explained.sum()\n",
    "\n",
    "        print(\"  Percent variance captured by all components:\",total_variance*100)\n",
    "        \n",
    "        paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "        for i in range(5):\n",
    "            print('  Component {}:'.format(i))\n",
    "            print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])\n",
    "\n",
    "    def get_paragraphs(self, corpus):\n",
    "        print('  Getting Novel paragraphs')\n",
    "        para = gutenberg.paras(corpus)\n",
    "        return para\n",
    "    \n",
    "    def clean_paragraphs(self, paras):\n",
    "        print('  Cleaning Paragraphs')\n",
    "        clean_paras = []\n",
    "        for paragraph in paras:\n",
    "            para = paragraph[0]\n",
    "            para = ' '.join(para)\n",
    "            para = nlpmodel.text_cleaner(para, False)\n",
    "            clean_paras.append(para)\n",
    "        \n",
    "        return clean_paras\n",
    "    \n",
    "    def tokenize_paragraphs(self, paras):\n",
    "        print('  Tokenizing Paragraphs')\n",
    "        token_paras = []\n",
    "        for para in paras:\n",
    "            token_paras.append(str(self.tokenize(para, False)))\n",
    "\n",
    "        return token_paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Bag of Words Analysis:\n",
      "  Getting Novel\n",
      "  Getting Novel\n",
      "  Running Text Cleaner\n",
      "  Running Text Cleaner\n",
      "  Tokenizing\n",
      "  Tokenizing\n",
      "  Getting Sentences\n",
      "  Getting Sentences\n",
      "  Running Bag of Words\n",
      "  Running Bag of Words\n",
      "  Setting Common Words\n",
      "  Running BoW Features\n",
      "    Processing row 0\n",
      "    Processing row 50\n",
      "    Processing row 100\n",
      "    Processing row 150\n",
      "    Processing row 200\n",
      "    Processing row 250\n",
      "    Processing row 300\n",
      "    Processing row 350\n",
      "    Processing row 400\n",
      "    Processing row 450\n",
      "    Processing row 500\n",
      "    Processing row 550\n",
      "    Processing row 600\n",
      "    Processing row 650\n",
      "    Processing row 700\n",
      "    Processing row 750\n",
      "    Processing row 800\n",
      "    Processing row 850\n",
      "    Processing row 900\n",
      "    Processing row 950\n",
      "    Processing row 1000\n",
      "    Processing row 1050\n",
      "    Processing row 1100\n",
      "    Processing row 1150\n",
      "    Processing row 1200\n",
      "    Processing row 1250\n",
      "    Processing row 1300\n",
      "    Processing row 1350\n",
      "    Processing row 1400\n",
      "    Processing row 1450\n",
      "    Processing row 1500\n",
      "    Processing row 1550\n",
      "    Processing row 1600\n",
      "    Processing row 1650\n",
      "    Processing row 1700\n",
      "    Processing row 1750\n",
      "    Processing row 1800\n",
      "    Processing row 1850\n",
      "    Processing row 1900\n",
      "    Processing row 1950\n",
      "    Processing row 2000\n",
      "    Processing row 2050\n",
      "    Processing row 2100\n",
      "    Processing row 2150\n",
      "    Processing row 2200\n",
      "    Processing row 2250\n",
      "    Processing row 2300\n",
      "    Processing row 2350\n",
      "    Processing row 2400\n",
      "    Processing row 2450\n",
      "    Processing row 2500\n",
      "    Processing row 2550\n",
      "    Processing row 2600\n",
      "    Processing row 2650\n",
      "    Processing row 2700\n",
      "    Processing row 2750\n",
      "    Processing row 2800\n",
      "    Processing row 2850\n",
      "    Processing row 2900\n",
      "    Processing row 2950\n",
      "    Processing row 3000\n",
      "    Processing row 3050\n",
      "    Processing row 3100\n",
      "    Processing row 3150\n",
      "    Processing row 3200\n",
      "    Processing row 3250\n",
      "    Processing row 3300\n",
      "    Processing row 3350\n",
      "    Processing row 3400\n",
      "    Processing row 3450\n",
      "    Processing row 3500\n",
      "    Processing row 3550\n",
      "    Processing row 3600\n",
      "    Processing row 3650\n",
      "    Processing row 3700\n",
      "    Processing row 3750\n",
      "    Processing row 3800\n",
      "    Processing row 3850\n",
      "    Processing row 3900\n",
      "    Processing row 3950\n",
      "    Processing row 4000\n",
      "    Processing row 4050\n",
      "    Processing row 4100\n",
      "    Processing row 4150\n",
      "    Processing row 4200\n",
      "    Processing row 4250\n",
      "    Processing row 4300\n",
      "    Processing row 4350\n",
      "    Processing row 4400\n",
      "    Processing row 4450\n",
      "    Processing row 4500\n",
      "    Processing row 4550\n",
      "    Processing row 4600\n",
      "    Processing row 4650\n",
      "    Processing row 4700\n",
      "    Processing row 4750\n",
      "    Processing row 4800\n",
      "    Processing row 4850\n",
      "    Processing row 4900\n",
      "    Processing row 4950\n",
      "    Processing row 5000\n",
      "    Processing row 5050\n",
      "    Processing row 5100\n",
      "    Processing row 5150\n",
      "    Processing row 5200\n",
      "    Processing row 5250\n",
      "    Processing row 5300\n",
      "  Training Random Forest Classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Blaine\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('    Training set score:', 0.9862068965517241, 'Test set score:', 0.8952067669172933)\n",
      "  Training Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Blaine\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('    Training set score:', 0.9579937304075236, 'Test set score:', 0.9579937304075236)\n",
      "  Training Boosted Classifier\n",
      "('    Training set score:', 0.886833855799373, 'Test set score:', 0.8735902255639098)\n",
      "Running tf-idf Analysis\n",
      "  Getting Novel paragraphs\n",
      "  Cleaning Paragraphs\n",
      "  Tokenizing Paragraphs\n",
      "  Vectorizer Built\n",
      "  Number of features: 1931\n",
      "  Original sentence: A very few minutes more , however , completed the present trial .\n",
      "  Tf_idf vector: {'minutes': 0.7127450310382584, 'present': 0.701423210857947}\n",
      "  Percent variance captured by all components: 92.93742258829678\n",
      "  Component 0:\n",
      "\" Oh !    0.999171\n",
      "\" Oh !    0.999171\n",
      "\" Oh !    0.999171\n",
      "\" Oh !    0.999171\n",
      "\" Oh !    0.999171\n",
      "\" Oh !    0.999171\n",
      "\" Oh !    0.999171\n",
      "\" Oh !    0.999171\n",
      "\" Oh !    0.999171\n",
      "\" Oh !    0.999171\n",
      "Name: 0, dtype: float64\n",
      "  Component 1:\n",
      "\" There were misunderstandings between them , Emma ; he said so expressly .                                                                                                 0.517622\n",
      "\" I do not know what your opinion may be , Mrs . Weston ,\" said Mr . Knightley , \" of this great intimacy between Emma and Harriet Smith , but I think it a bad thing .\"    0.491687\n",
      "\" So do I ,\" said Mrs . Weston gently , \" very much .\"                                                                                                                      0.467199\n",
      "\" You are right , Mrs . Weston ,\" said Mr . Knightley warmly , \" Miss Fairfax is as capable as any of us of forming a just opinion of Mrs . Elton .                         0.452887\n",
      "\" You have made her too tall , Emma ,\" said Mr . Knightley .                                                                                                                0.445046\n",
      "\" Are you well , my Emma ?\"                                                                                                                                                 0.419296\n",
      "Emma demurred .                                                                                                                                                             0.419296\n",
      "\" Emma , my dear Emma \"                                                                                                                                                     0.414196\n",
      "\" No great variety of faces for you ,\" said Emma .                                                                                                                          0.404235\n",
      "\" In one respect , perhaps , Mr . Elton ' s manners are superior to Mr . Knightley ' s or Mr . Weston ' s .                                                                 0.402629\n",
      "Name: 1, dtype: float64\n",
      "  Component 2:\n",
      "\" Ah !      0.993265\n",
      "\" Ah !      0.993265\n",
      "\" Ah !      0.993265\n",
      "\" Ah !      0.993265\n",
      "\" Ah !      0.993265\n",
      "But ah !    0.993265\n",
      "\" Ah !      0.993265\n",
      "\" Ah !      0.993265\n",
      "\" Ah !      0.993265\n",
      "But ah !    0.993265\n",
      "Name: 2, dtype: float64\n",
      "  Component 3:\n",
      "\" There were misunderstandings between them , Emma ; he said so expressly .    0.649717\n",
      "Emma demurred .                                                                0.579099\n",
      "\" Are you well , my Emma ?\"                                                    0.579099\n",
      "\" Emma , my dear Emma \"                                                        0.562079\n",
      "\" No great variety of faces for you ,\" said Emma .                             0.469376\n",
      "\" What is to be done , my dear Emma ? what is to be done ?\"                    0.456794\n",
      "\" But you will come again ,\" said Emma .                                       0.438116\n",
      "\" From something that he said , my dear Emma , I rather imagine \"              0.425465\n",
      "\" Yes ,\" said Emma , \" I hope I do .\"                                          0.393618\n",
      "The speech was more to Emma than to Harriet , which Emma could understand .    0.365376\n",
      "Name: 3, dtype: float64\n",
      "  Component 4:\n",
      "\" But Miss Bates and Miss Fairfax !\"                                                                                            0.660397\n",
      "To Miss                                                                                                                         0.601330\n",
      "\" You and Miss Smith , and Miss Fairfax , will be three , and the two Miss Coxes five ,\" had been repeated many times over .    0.549041\n",
      "\" Dear Miss Woodhouse !\"                                                                                                        0.512698\n",
      "\" What can it be , Miss Woodhouse ? what can it be ?                                                                            0.480709\n",
      "For Miss , read Miss Smith .                                                                                                    0.447759\n",
      "\" And when is Miss Fairfax to leave you ?\"                                                                                      0.447009\n",
      "\" You are acquainted with Miss Jane Fairfax , sir , are you ?\"                                                                  0.439937\n",
      "\" Come Miss Woodhouse , Miss Otway , Miss Fairfax , what are you all doing ? Come Emma , set your companions the example .      0.438338\n",
      "\" Thank you , dear Miss Woodhouse .                                                                                             0.438040\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "nlpmodel = NLPModel()\n",
    "nlpmodel.run_bow('carroll-alice.txt', 'Carroll', 'austen-persuasion.txt', 'Austen')\n",
    "nlpmodel.run_tfidf('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
